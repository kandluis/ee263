\documentclass[12pt]{exam}

\usepackage[utf8]{inputenc}  % For UTF8 source encoding.
\usepackage{amsmath}  % For displaying math equations.
\usepackage{amsfonts} % For mathematical fonts (like \mathbb{E}!).
\usepackage{upgreek}  % For upright Greek letters, such as \upvarphi.
\usepackage{wasysym}  % For additional glyphs (like \smiley!).
\usepackage{mathrsfs} % For script text (hash families and universes).
\usepackage{enumitem}
\usepackage{graphicx}
% For document margins.
\usepackage[left=.8in, right=.8in, top=1in, bottom=1in]{geometry}
\usepackage{lastpage} % For a reference to the number of pages.
\usepackage[table,xcdraw]{xcolor}
\usepackage{pdfpages}
\usepackage{verbatim}

% TODO: Enter your name here :)
\newcommand*{\authorname}{Luis A. Perez}

\newcommand*{\duedate}{Wednesday, July 17th}
\newcommand*{\duetime}{11:59 pm}

% Fancy headers and footers
\headrule
\firstpageheader{EE 263\\Summer 2019}{Homework 3 \\ }{Due: \duedate\\at \duetime}
\runningheader{EE 263}{Homework 3}{\authorname}
\footer{}{\footnotesize{Page \thepage\ of \pageref{LastPage}}}{}

% Exam questions.
\newcommand{\Q}[1]{\question{\large{\textbf{#1}}}}
\qformat{}  % Remove formatting from exam questions.

% Useful macro commands.
\newcommand*{\bigtheta}[1]{\Theta\left( #1 \right)}
\newcommand*{\bigo}[1]{O \left( #1 \right)}
\newcommand*{\bigomega}[1]{\Omega \left( #1 \right)}
\newcommand*{\prob}[1]{\text{Pr} \left[ #1 \right]}
\newcommand*{\ex}[1]{\text{E} \left[ #1 \right]}
\newcommand*{\var}[1]{\text{Var} \left[ #1 \right]}

\newcommand*{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand*{\HH}{\mathscr{H}}   % Family of hash functions.
\newcommand*{\UU}{\mathscr{U}}   % Universe.
\newcommand*{\eps}{\varepsilon}  % Epsilon.


% Custom formatting for problem parts.
\renewcommand{\thepartno}{\roman{partno}}
\renewcommand{\partlabel}{\thepartno.}

% Framed answers.
\newcommand{\answerbox}[1]{
\begin{framed}
\hspace{\fill}
\vspace{#1}
\end{framed}}

\printanswers

\setlength\answerlinelength{2in} \setlength\answerskip{0.3in}

\begin{document}
\title{EE 263 Homework 3}
\author{\authorname}
\date{}
\maketitle
\thispagestyle{headandfoot}
\setcounter{MaxMatrixCols}{15}

\begin{questions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Q{Fitting a model for hourly temperature}

  \begin{solution}
    \begin{enumerate}[label=(\alph*)]
      \item In order to find $a \in \mathbb{R}$ and $p \in \mathbb{R}^N$ (which is 24-periodic) that minimizes the RMS value of $y - \hat{y}$, we can rephrase our original predictor model as a linear system:
      $$
        \hat{y} = Ax
      $$
      where $\hat{y} \in \mathbb{R}^N$ and $x \in \mathbb{R}^{25}$, which represents our parameters (since $p$ is 24-periodic). More precisely, we have:
      \[
        x =
          \begin{bmatrix}
            p_{24} \\
            p_{23} \\
            \vdots \\
            p_2 \\
            p_1 \\
            a
          \end{bmatrix} \in \mathbb{R}^25
      \]
      What is the same of $A$. In fact, we have the following:
      \[
        A =
          \begin{bmatrix}
            1 & 0 & 0 & \cdots & 0 & N \\
            0 & 1 & 0 & \cdots & 0 & N - 1 \\
            0 & 0 & 1 & \cdots & 0 & N - 2 \\
            \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
            0 & 0 & 0 & \cdots & 1 & N - 23 \\
            1 & 0 & 0 & \cdots & 0 & N - 24 \\
            0 & 1 & 0 & \cdots & 0 & N - 25 \\
            0 & 0 & 1 & \cdots & 0 & N - 26 \\
            \vdots & \vdots & \vdots & \ddots & \vdots & \vdots 
          \end{bmatrix} \in \mathbb{R}^{N \times 25}
      \]
      This means that $A$ is a skinny and tall matrix, for an over-constrained system of equations. Finding the $x$ that minimizes the RMS of $\hat{y} - y$ can be done by computing:
      \[
        x = (A^TA)^{-1}A^Ty
      \]
      The $x$ above give use $p$ as well as $a$. 

      \item We now perform the process described in part (a). The trend parameter is:
      \[
        a=-0.012075460503471858
      \]
      A plot of the predictions as well as the observed values can be seen in Figure \ref{fig:temperatures_on_train}.

      \item 
        The RMSE of the prediction error for tomorrow's temperatures is: 
        \[
          0.6521628280735887
        \]

        A plot of the predicted and observed temperatures for tomorrow can be seen in Figure \ref{fig:temperatures_on_test}.
    \end{enumerate}
  \end{solution}

  \begin{figure}[!ht]
    \centering
    \includegraphics{figures/temps_on_train.png}
    \caption{Plot of predicted (x) and observed (o) temperatures on training data.}
    \label{fig:temperatures_on_train}
  \end{figure}
  \begin{figure}[!ht]
    \centering
    \includegraphics{figures/temps_on_test.png}
    \caption{Plot of predicted (x) and observed (o) temperatures on test data.}
    \label{fig:temperatures_on_test}
  \end{figure}

\newpage
\Q{Identifying a system from input/output data}

  \begin{solution}
    \begin{enumerate}[label=(\alph*)]
      \item We wish to find $A$ such that:
       \[
        J = \sum_{k=1}^N || Ax^{(k)} - y^{(k)}||^2
       \]
       is minimized. Taking the derivative wrt. $A$, we have:
       \begin{align*}
        \frac{\partial J}{\partial A} &= \frac{\partial}{\partial A} \left[\sum_{k=1}^N || Ax^{(k)} - y^{(k)}||^2 \right ] \\
        &= \sum_{k=1}^N \frac{\partial}{\partial A}\left[ || Ax^{(k)} - y^{(k)}||^2 \right ] \\
        &= \sum_{k=1}^N \frac{\partial}{\partial A}\left[ (Ax^{(k)} - y^{(k)})^T(Ax^{(k)} - y^{(k)}) \right ] \\
        &= 2\sum_{k=1}^N  (Ax^{(k)} - y^{(k)}) x^{(k)T} \\
        &= 2A \sum_{k=1}^N x^{(k)}x^{(k)T} - 2\sum_{k=1}^N  y^{(k)}x^{(k)T}
       \end{align*}
       So setting equal to $0$ and solving for $A$, we have:
       \begin{align*}
        A &= \left(\sum_{k=1}^N  y^{(k)}x^{(k)T}\right)\left(\sum_{k=1}^N x^{(k)}x^{(k)T} \right)^{-1}
       \end{align*}
       So we can find $A$ if and only if the second term above is invertible.

       \item Implementing the method discussed above, we obtain the following $A$:
       \[
        \hat{A} =
          \begin{bmatrix}
            2.02992454 &  5.02077879 &  5.01040266 \\
            0.0114300076 &  6.99991043 &  1.01061265 \\
            7.04239020 & 0 &  6.94476335 \\
            6.99765743 &  3.97592792 &  4.00242122 \\
            9.01295285 &  1.04493868 &  6.99800225 \\
            4.01187599 &  3.96488792 &  9.02674982 \\
            4.98710794 &  6.97233996 &  8.03363399 \\
            7.94249406 &  6.08754514 &  3.01735388 \\
            0 &  8.97218370 & -0.0385465462 \\
            1.06123427 &  8.02076138 &  7.02847693
          \end{bmatrix}
       \]
       This gives us a realtive error of $0.05814323689487761$.
    \end{enumerate}
  \end{solution}

\newpage
\Q{Robust regression using the Huber penalty function}

  \begin{solution}
    \begin{enumerate}[label=(\alph*)]
      \item This is a straight-forward application of weighed iterative least squares. In particular, our weight function is given by the below at iteration $k + 1$:

      \[
        w_i(a^{(k)},b^{(k)}) = \frac{H_{\delta}(a^{(k)}t_i + b^{(k)} - x_i)}{(a^{(k)}t_i + b^{(k)} - x_i)^2}
      \]
      We have to be a bit careful with the denominator to make sure it does not equal $0$. Once we have the weights defined, we can use the standard update equation given by:
      \[
          \begin{bmatrix}
            a^{(k+1)} \\
            b^{(k+1)}
          \end{bmatrix} = (A^TW(a^{(k)},b^{(k)})A)^{-1}A^TW(a^{(k)},b^{(k)})x
      \]
      where we have:
      \begin{align*}
        A &= \begin{bmatrix}
          t_1 & 1 \\
          \vdots & \vdots \\
          t_N & 1
        \end{bmatrix} \in \mathbb{R}^{N \times 2} \\
        W &= \begin{bmatrix}
          w_1(a, b) & 0 & \cdots & 0 \\
          0 & w_2(a,b) & \cdots & 0 \\
          \vdots & \vdots & \ddots & \vdots \\
          0 & 0 & \cdots & w_N(a,b)
        \end{bmatrix} \in \mathbb{R}^{N \times N}
      \end{align*}
    \item Applying the method above to ``huber\_penalty\_function\_data.m'', we arrive at the following parameter values with the Huber loss:
    \[
      a = -4.9920376, b = 23.5669247
    \]
    for a total Huber loss of $0.40706964531961987$.
    

    As for ordinary least squares, we have the following parameters:
    \[
      a = -3.94855818, b = 19.03697567
    \]
    A visualization of the results in presented in Figure \ref{fig:huber_fit}. We can see that the red line (Huber fit) matches the main data much more closely, since it is penalized less for the outliers. On the other hand, we see that the LS fit (green line) has to change slope and shift to try and accomodate the outlines, meaning the main datapints suffer.
    \end{enumerate}
  \end{solution}

  \begin{figure}
    \centering
    \includegraphics{line_plots_for_huber_fit.png}
    \caption{Plot of data as well as LS and Huber Fit}
    \label{fig:huber_fit}    
  \end{figure}

\newpage
\Q{Estimating a signal with interference}

  \begin{solution}
    \begin{enumerate}[label=(\alph*)]
      \item We now formalize each of the methods described:
        \begin{itemize}
          \item In this process, the estimate $\hat{x}$ is given by:
            \[
              \hat{x} = (A^TA)^{-1}A^Ty
            \]
            since it's just a straight-forward over-determined least squares problem.
          \item This methods consists of estimated both $\hat{x}$ and $\hat{v}$. Basically, this involves rewriting the problem $y = Ax + Bv$ as a single matrix, like below:
          \begin{align*}
            y &= Ax + Bv \\
            &= \begin{bmatrix} A & B \end{bmatrix}\begin{bmatrix} x \\ v \end{bmatrix} \\
            &= A'x'
          \end{align*}
          So now that we've presented it in this way, we have the solutions as:
          \[
            \begin{bmatrix}
              \hat{x} \\
              \hat{v}
            \end{bmatrix} = (A'^TA')^{-1}A'^Ty
          \]
          \item This method follows the same set-up as the method above, so we re-use $A' \in \mathbb{R}^{m \times (n + p)}$ and $x' \in \mathbb{R}^{(n+p)}$. Using the same process as described, we find $\hat{x}^{(1)}$ and $\hat{v}$.

          We then form the pseudo-measurement $\hat{y} = y - B\hat{v}$. We now have the system:
          \[
            \hat{y} = Ax
          \]
          and now find our \textit{true} estimate. Since this is just standard LS, we have:
          \[
            \hat{x} = (A^TA)^{-1}A^T\hat{y}
          \]
        \end{itemize}
      \item Our first claim is that all three mothods are actually equivalent. We beging by showing what we consider is easiests first. We show that Miki and Nikola's methods lead to the same answer. 

      Let's start by simplifying the most complex (Miki's) method.
      Subtituting what we know $\hat{y}$ to be, we get:
      \begin{align*}
        \hat{x} &= (A^TA)^{-1}A^T(y - B\hat{v}) \tag{Subtituting $\hat{y}$} \\
        &= (A^TA)^{-1}A^Ty - (A^TA)^{-1}A^TB\hat{v} \tag{Distributing} \\
        &= (A^TA)^{-1}A^Ty
      \end{align*}
      The most interesting part here is the last line. How did we get rid of the second term? To see this, let us consider each entry of the vector $z = A^TB\hat{v} \in \mathbb{R}^{n}$. 
      \begin{align*}
        z_i &= a_i^T (B\hat{v}) \tag{Where $a_i$ is the $i$-th column of $A$} \\
        &= a_i^Tb \tag{Where $b \in \textbf{img}(A)$} \\
        &= 0 \tag{$\textbf{img}(A) \cap \textbf{img}(B) = \{ 0\}$}
      \end{align*}
      Where the last-line is known becase $a_i$ (a column of $A$, so $a_i$ must be in the image of $A$) is orthogonal to $z$ (a vector in the image of $B$) since the images of $A$ and $B$ don't intersect. 

      From the above, we see therefore that Miki's method actually finds $\hat{x}$ which is equivalent to the one found by Nikola's method.

      With that complete, let's simplify the final remaining method (Almir's). We have:
      \begin{align*}
        \begin{bmatrix} 
          \hat{x} \\ \hat{v}
        \end{bmatrix} &= (A'^TA')^{-1}A'^Ty \tag{Almir's method} \\
        &= \left( \begin{bmatrix}
          A^T \\ 
          B^T
        \end{bmatrix} 
        \begin{bmatrix} A & B \end{bmatrix} \right)^{-1}
        \begin{bmatrix}
          A^T \\ 
          B^T
        \end{bmatrix}y \tag{Subtituing $A'$} \\
        &= \begin{bmatrix}
          A^TA & A^TB \\ 
          B^TA & B^TB
        \end{bmatrix}^{-1}
        \begin{bmatrix}
          A^Ty \\ 
          B^Ty
        \end{bmatrix} \tag{Multiplying a few things out} \\
        &= \begin{bmatrix}
          A^TA & 0 \\ 
          0 & B^TB
        \end{bmatrix}^{-1}
        \begin{bmatrix}
          A^Ty \\ 
          B^Ty
        \end{bmatrix} \tag{$A^TB = B^TA = 0$ since $\textbf{img}(A) \cap \textbf{img}(B) = \{ 0 \}$} \\
        &= \begin{bmatrix}
          (A^TA)^{-1} & 0 \\ 
          0 & (B^TB)^{-1}
        \end{bmatrix}
        \begin{bmatrix}
          A^Ty \\ 
          B^Ty
        \end{bmatrix}  \tag{Inverse of block-diagonal matrix. $A,B$ are full-rank} \\
        \implies \hat{x} &= (A^TA)^{-1}A^Ty
      \end{align*}
      As such, even Almir's method is the same, mostly because we know that the images of $A$ and $B$ don't intersect.
      \item The mothods are all the same. Still, probably worth-it to go with the simplest method, which is Nikolas \textbf{ignore and estimate} method.
    \end{enumerate}
  \end{solution}

\end{questions}

\includepdf[
    %% Include all pages of the PDF
    pages=-,
    %% make this page have the usual page style
    %% (you can change it to plain etc). By default pdfpages
    %% sets the pagecommand to \pagestyle{empty}
    pagecommand={\pagestyle{headings}}]
%% The pdf file itself
{HW4Code.pdf}















\end{document}
