\documentclass[12pt]{exam}

\usepackage[utf8]{inputenc}  % For UTF8 source encoding.
\usepackage{amsmath}  % For displaying math equations.
\usepackage{amsfonts} % For mathematical fonts (like \mathbb{E}!).
\usepackage{upgreek}  % For upright Greek letters, such as \upvarphi.
\usepackage{wasysym}  % For additional glyphs (like \smiley!).
\usepackage{mathrsfs} % For script text (hash families and universes).
\usepackage{enumitem}
\usepackage{graphicx}
% For document margins.
\usepackage[left=.8in, right=.8in, top=1in, bottom=1in]{geometry}
\usepackage{lastpage} % For a reference to the number of pages.
\usepackage[table,xcdraw]{xcolor}
\usepackage{pdfpages}
\usepackage{verbatim}

% TODO: Enter your name here :)
\newcommand*{\authorname}{Luis A. Perez}

\newcommand*{\duedate}{Wednesday, July 17th}
\newcommand*{\duetime}{11:59 pm}

% Fancy headers and footers
\headrule
\firstpageheader{EE 263\\Summer 2019}{Homework 3 \\ }{Due: \duedate\\at \duetime}
\runningheader{EE 263}{Homework 3}{\authorname}
\footer{}{\footnotesize{Page \thepage\ of \pageref{LastPage}}}{}

% Exam questions.
\newcommand{\Q}[1]{\question{\large{\textbf{#1}}}}
\qformat{}  % Remove formatting from exam questions.

% Useful macro commands.
\newcommand*{\bigtheta}[1]{\Theta\left( #1 \right)}
\newcommand*{\bigo}[1]{O \left( #1 \right)}
\newcommand*{\bigomega}[1]{\Omega \left( #1 \right)}
\newcommand*{\prob}[1]{\text{Pr} \left[ #1 \right]}
\newcommand*{\ex}[1]{\text{E} \left[ #1 \right]}
\newcommand*{\var}[1]{\text{Var} \left[ #1 \right]}

\newcommand*{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand*{\HH}{\mathscr{H}}   % Family of hash functions.
\newcommand*{\UU}{\mathscr{U}}   % Universe.
\newcommand*{\eps}{\varepsilon}  % Epsilon.


% Custom formatting for problem parts.
\renewcommand{\thepartno}{\roman{partno}}
\renewcommand{\partlabel}{\thepartno.}

% Framed answers.
\newcommand{\answerbox}[1]{
\begin{framed}
\hspace{\fill}
\vspace{#1}
\end{framed}}

\printanswers

\setlength\answerlinelength{2in} \setlength\answerskip{0.3in}

\begin{document}
\title{EE 263 Homework 3}
\author{\authorname}
\date{}
\maketitle
\thispagestyle{headandfoot}
\setcounter{MaxMatrixCols}{15}

\begin{questions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Q{Memory of a linear dynamical, time-invariant system}

  \begin{solution}
    \begin{enumerate}[label=(\alph*)]
      \item Let us first consider how we might check if a valid impose response of fixed size $M$ exists. The first thing to note is that the covolution operator given in the problem statent can actually be written as a linear system:
      \[
        \bar{y} = Ah
      \]
      where $\bar{y} \in \mathbb{R}^{T - M}, h \in \mathbb{R}^{M}$ and $A \in \mathbb{R}^{(T-M) \times M}$. Note that we remove the first $\{y_1, \cdots, y_{M}\}$. In fact, we can use the matrix $A$ as defined below:
      \begin{align*}
        A &=
          \begin{bmatrix}
          u_M & u_{M-1} & u_{M-2} & \cdots & u_1 \\ 
          u_{M+1} & u_{M} & u_{M-1} & \cdots & u_2 \\ 
          u_{M+2} & u_{M + 1} & u_{M} & \cdots & u_2 \\ 
          \vdots & \vdots & \vdots & \ddots \\
          u_{T-2} & u_{T-3} & u_{T-4} & \cdots & u_{T-M-1} \\
          u_{T-1} & u_{T-2} & u_{T-3} & \cdots & u_{T-M} \\
          \end{bmatrix} \\
        y_{-1} &= 
          \begin{bmatrix}
            y_{M+1} \\
            \vdots \\
            y_T
          \end{bmatrix} \\
        h &= 
          \begin{bmatrix}
            h_1 \\
            \vdots \\
            h_M
          \end{bmatrix}
      \end{align*}
      We can see by inspection above that $Ah$ performs the needed convolutions betweens $u$ and $h$ to obtain $\bar{y}$, by the properties of matrix multiplication (eg, to obtain $y_i$, we compute the dot product of the $i-M$-th row of $A$ with $h$, which is exactly what our convolution dictates, and for $i \leq M$, the convolution is not fully defined so we ignore them).

      In the problem statement, we're given the fact that $T > 2M$, so this means that there will always be at least $T-M > 2M -M = M$ rows in $A$, meaning it will always be a tall and skinny matrix. 

      As such, we can use the pseudo inverse to find the closests solution for $h$, computing:
      \[
        \bar{h} = (A^TA)^{-1}A^T\bar{y}
      \]
      Finally, once we've computed this $h$, we can re-compute that $\bar{y}$ given our inputs forming $A$, and see if this equals our what we started with. In other words, we have that:
      \[
        ||A\bar{h} - \bar{y}|| \leq \epsilon \implies M \text{ is a valid value} \tag{$\epsilon$ is needed to deal with floating point imprecision}
      \]
      The above gives us a way to check if $M$ is sufficient. To finallize our method, we simply iterate over the possible values of $M$ from $M = 1, \cdots \frac{T}{2} - 1$ in order until we find a valid value.

      \item Applying the process we described above, we find that $M = 7$ is the smallest value that works. We also have:
      \[
        h =
          \begin{bmatrix}
            0.63 \\
            0.27 \\
            0.02 \\
            0.37 \\
            0.96 \\
            0.95 \\
            0.46
          \end{bmatrix}
      \]
    \end{enumerate}
  \end{solution}

\newpage
\Q{Norm preserving implies orthonormal columns}

  \begin{solution}
    We show that if $A \in \mathbb{R}^{m \times n}$ satifies $|| Ax|| = ||x||$ for all $x \in \mathbb{R}^n$, then $A$ has orthonormal columns.

    This boils down to showing that $A^TA = I$, which can be broken into two parts. Let $A_i$ be the $i$-th column of $A$. Then we need to show that $A_i^TA_i = 1$, and that $A_i^TA_j = 0$ for $i \neq j$. Let us consider the former first:

    \begin{align*}
      A_i^TA_i &= (Ae_i)^T(Ae_i) \tag{Multiplying $A$ by $e_i$ extracts the $i$-th column} \\
      &= ||Ae_i||^2 \tag{Definition of dot product treating $Ae_i$ as a vector} \\
      &= ||e_i||^2 \tag{Multiplication by $A$ preserves the norm} \\
      &= 1
    \end{align*}
    Let us now consider the latter case. We follow the hint:
    \begin{align*}
    ||A(e_i + e_j)||^2 &= ||e_i + e_j|| \tag{$A$ preserves norm} \\
    &= 2
    \end{align*}
    However, we also have that:
    \begin{align*}
    ||A(e_i + e_j)||^2 &= ||Ae_i + Ae_j||^2 \tag{Linearity of $A$} \\
    &= ||Ae_i||^2 + ||Ae_j||^2 + 2(Ae_i)^T(Ae_j) \tag{Definition of vector norm} \\
    &= 2 + 2(Ae_i)^T(Ae_j) \tag{Using previous results}
    \end{align*}
    Putting the two results together, we must have that:
    \begin{align*}
      2 + 2(Ae_i)^T(Ae_j) = 2 \implies 2(Ae_i)^T(Ae_j) = 0 \implies (Ae_i)^T(Ae_j) = 0 \implies A_i^TA_j = 0
    \end{align*}
    As such, we've now shown that $A^TA = I$.
  \end{solution}


\newpage
\Q{Sensor integrity monitor}
  \begin{solution}
    We need to find $k \in \mathbb{R}^{k \times m}$ such that:
      \begin{itemize}
        \item $By = 0$ for any $y$ which is consistent.
        \item $By \neq 0$ for any $y$ which is inconsistent.
      \end{itemize}

    The above can be summarized as follow. Requirement (1) means that given $y \in \textbf{Img}(A)$, we must have $By = 0$. This means that $\textbf{Img}(A) \subseteq \textbf{Ker}(B) $.

    The second requirement specifies that for any $y \notin \textbf{Img}(A)$, we must have $By \neq 0$. Phrase another way, this says that if $By = 0$, it must be in the $\textbf{Img}(A)$. So we have $\textbf{Ker}(B) \subseteq \textbf{Img}(A)$.

    As such, we're really just tring to construct a matrix $B$ whose kernal is equal to the span of of the columns of $A$. This is actually a rather straight-forward process as long as we recall the following theorem, which holds for \textit{any} matrix $C$:

    \[
      \textbf{Ker}(C^T) = \textbf{Img}(C)^{\perp}
    \]
    Thinking about it step-by-step, if the Kernel of $B$ is equal to the Image of $A$, it must mean that the Image of $B$ is equal to the complement of the Image of $A$. Using the formula presented abouve, the completement of the Image of $A$ is the null-space of of $A^T$.

    So simply put, we just need to (1) find the $M-N$ vectors spanning the nullspace of $A^T$ and take these to be the rows of $B$. This will guarantee that the Kernel of $B$ will equal the image of $A$.

    Doing exactly the process described above gives us $B \in \mathbb{R}^{2 \times 5}$ as follows:

    \[
      B =
        \begin{bmatrix}
          0 & -5.47259193 &  2 &  8.47259193 & 1 \\
          0 & 6.97831636 & 6.39848918 & 2.61941742 & 3.19924459
        \end{bmatrix}
    \]


  \end{solution}

\newpage
\Q{Sensor integrity monitor}
  \begin{solution}
    \begin{enumerate}[label=(\alph*)]
      \item TODO
    \end{enumerate}
  \end{solution}

\newpage
\Q{Solving linear equations via QR Factorization}

  \begin{solution}
    The problem statement essentially boils down to solving the system $Rx = z$ where $R \in \mathbb{R}^{n \times n}$ is upper triangular and non-singular (eg, $R_{ii} \neq 0, \forall i$) and $z,x \in \mathbb{R}^n$.

    The algoritm proceeds as follows:
    \begin{enumerate}
      \item First, let us find $x_n$. Let $R_i \in \mathbb{R}^{1 \times n}$ be the $i$-th row of $R$ as a row vector. We have:
        \begin{align*}
          z_n &= R_n x \\
          &= \sum_{i=1}^n R_{ni} x_i \\
          &= R_{nn} x_n \tag{Since $R$ is upper triangular, $R_{ni} = 0$ for $i < n$} \\
          \implies x_n &= \frac{z_n}{R_{nn}} \tag{Note this is valid since $R_{nn} \neq 0$}
        \end{align*}
      \item Using a similar process, we can now find $x_{n-1}$. We have:
        \begin{align*}
          z_{n-1} &= R_{n-1} x \\
          &= \sum_{i=1}^n R_{n-1,i} x_i \\
          &= R_{n-1, n-1}x_{n-1} + R_{n-1,n} x_n \tag{Since $R$ is upper triangular, $R_{n-1,i} = 0$ for $i < n-1$} \\
          \implies x_{n-1} &= \frac{z_n - R_{n-1,n}x_n}{R_{n-1,n}} \tag{Note this is valid since $R_{n-1,n} \neq 0$}
        \end{align*}
        While the equation above looks more complicated, we note that all of the variables in the RHS are known (specifically, we found the value of $x_n$ previously).
      \item We simply continue the process from above. Let us assume that we've found the values $x_k$ for all $k > i$. Then to compute $x_i$ (the next value), we have:
        \begin{align*}
          z_i &= R_i x \\
          &= \sum_{j=1}^n R_{ij} x_j \\
          &= \sum_{j=i}^n R_{ij} x_j\tag{Since $R$ is upper triangular, $R_{ij} = 0$ for $j < i$} \\
          \implies x_i &= \frac{z_i - \sum_{j=i+1}^n R_{ij} x_j}{R_{ii}} \tag{Note this is valid since $R_{ii} \neq 0$}
        \end{align*}
        We know $z_i, R_{ij}, \forall j$, and by our assumption, we know $x_k, \forall k > i$, so we know all the variables in the LHS. As such, it's relatively straight-forwrad to compute $x_i$.
    \end{enumerate}

    As such, we can see that the $x_n, x_{n-1}, \cdots, x_1$ produced by our algorithm are valid solutions to the system $z = Rx$. Furthermore, the algorithm cannot fail since $R$ is non-singular (all diagonal entries are non-zero).
  \end{solution}

\newpage
\Q{Quadratic extrapolation of a time series, using least-squares fit}

  \begin{enumerate}[label=(\alph*)]
    \item In the problem statement, we are given that $\hat{z}(t+1) = f(t+1)$ where $f(\tau) = a_2\tau^2 + a_1 \tau + a_0$ is the quadratic function which minimizes:
    \[
      J = \sum_{\tau = t-9}^t (z(\tau) - f(\tau))^2
    \]
    We can rewrite the above in matrix form as follows:
    \[
      J = ||Fa - z||^2
    \]
    where 
    \begin{align*}
      z &=
        \begin{bmatrix}
          z(t) \\
          z(t-1) \\
          \vdots \\
          z(t-9)
        \end{bmatrix} \\
      a &= 
        \begin{bmatrix}
          a_1 \\
          a_2 \\
          a_3
        \end{bmatrix} \\
      F &=
        \begin{bmatrix}
          1 & t & t^2 \\
          1 & t - 1 & (t-1)^2 \\
          \vdots & \vdots & \vdots \\
          1 & t - 9 & (t - 9)^2
        \end{bmatrix}
    \end{align*}
    If we want to solve for $a$, we can immediately arrive at the solution (based on what we've covered in lecture):
    \[
      a = (F^TF)^{-1}F^Tz
    \]
    With the above, we can write $\hat{z}(t+1)$ as follows:
    \begin{align*}
      \hat{z}(t+1) &= f(t+1)  \\
      &= a_1 + a_2(t+1) + a_3(t+1)^2 \\
      &=
        \begin{bmatrix}
          1 & t + 1 & (t+1) 
        \end{bmatrix}
        \begin{bmatrix}
          a_1 \\
          a_2 \\
          a_3
        \end{bmatrix}\\
      &= 
        \begin{bmatrix}
          1 & t + 1 & (t+1) 
        \end{bmatrix}(F^TF)^{-1}F^Tz
    \end{align*}
    As such, we can see that for a given value of $t$, we have:
    \[
      \hat{z}(t+1) =
        \begin{bmatrix}
          1 & t + 1 & (t+1)^2
        \end{bmatrix} (F^TF)^{-1}F^T
        \begin{bmatrix}
          z(t) \\
          z(t-1) \\
          \vdots \\
          z(t-9)
        \end{bmatrix}
    \]
    which implies that we should have:
    \[
      c =  \begin{bmatrix}
          1 & t + 1 & (t+1)^2
        \end{bmatrix} (F^TF)^{-1}F^T
    \]
    which at first glance appears to depend on the value of $t$. However, using code to solve the system symbolically, we see that the result is:
    \[
      c
    \]
  \end{enumerate}
\end{questions}






















\end{document}
